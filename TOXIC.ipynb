{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Kr38icA-qsR",
        "outputId": "c1b2254b-a4dc-4f16-f522-56ec43af32e5"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HreKaRmU-r9_"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm , trange\n",
        "from scipy import stats\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9tWBRah-sBD"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5964_AIM-sDn"
      },
      "source": [
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\", do_lower_case=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYQidgSY-mY2",
        "outputId": "1d5751dd-edd9-4b98-8f1f-13c81d1e372c"
      },
      "source": [
        "print(transformers.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVCMCYqD-sF7"
      },
      "source": [
        "class TOXICDATASET:\n",
        "    def __init__(self,comment_text,target,tokenizer,max_len):\n",
        "        self.comment_text = comment_text\n",
        "        self.target = target\n",
        "        self.tokenizer = tokenizer \n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.comment_text)\n",
        "    \n",
        "    def __getitem__(self,item):\n",
        "      comment_text = str(self.comment_text[item])\n",
        "      comment_text = \" \".join(comment_text.split())\n",
        "        \n",
        "      inputs = self.tokenizer.encode_plus(comment_text,\n",
        "                                          None,\n",
        "                                          add_special_tokens = True,\n",
        "                                          max_length = self.max_len,\n",
        "                                          truncation=True)      \n",
        "      ids = inputs[\"input_ids\"]\n",
        "      masks = inputs[\"attention_mask\"]\n",
        "      token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "      padding_length = self.max_len - len(ids)\n",
        "      ids = ids + ([0] * padding_length)\n",
        "      masks = masks + ([0] * padding_length)\n",
        "      token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "\n",
        "\n",
        "      return {\n",
        "          \"input_ids\" : torch.tensor(ids,dtype = torch.long),\n",
        "          \"attention_masks\" : torch.tensor(masks,dtype = torch.long),\n",
        "          \"token_type_ids\" : torch.tensor(token_type_ids,dtype = torch.long),\n",
        "          \"targets\" : torch.tensor(self.target[item],dtype = torch.float)\n",
        "      }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfXn5IL9-sI4"
      },
      "source": [
        "class TOXICMODEL(nn.Module):\n",
        "    def __init__(self,conf):\n",
        "        super(TOXICMODEL,self).__init__()\n",
        "        self.conf = conf\n",
        "        self.bert = transformers.BertModel.from_pretrained(self.conf)\n",
        "        self.dropout = torch.nn.Dropout(p = 0.3)\n",
        "        self.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "    def forward(self,input_ids,attention_mask,token_type_ids):\n",
        "        _, output = self.bert(input_ids,attention_mask,token_type_ids,return_dict = False)\n",
        "        output = self.dropout(output)\n",
        "        output = self.classifier(output)   \n",
        "        return output "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o_TqwqI-sLC"
      },
      "source": [
        "def Loss_Func(output,targets):\n",
        "  return nn.BCEWithLogitsLoss()(output,targets)\n",
        "\n",
        "\n",
        "def Train_Func(dataLoader,model,optimizer,device,scheduler = None):\n",
        "    model.train()\n",
        "\n",
        "    for index,batch in enumerate(dataLoader):\n",
        "      ids = batch[\"input_ids\"]\n",
        "      masks = batch[\"attention_masks\"]\n",
        "      token = batch[\"token_type_ids\"]\n",
        "      target = batch[\"targets\"]\n",
        "\n",
        "      ids = ids.to(device,dtype = torch.long)\n",
        "      masks = masks.to(device,dtype = torch.long)\n",
        "      token = token.to(device,dtype = torch.long)\n",
        "      target = target.to(device,dtype = torch.float)\n",
        "              \n",
        "      optimizer.zero_grad()\n",
        "      output = model(input_ids = ids,\n",
        "                    attention_mask = masks,\n",
        "                    token_type_ids = token)\n",
        "      loss = Loss_Func(output,target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if scheduler is not None:\n",
        "        scheduler.step() \n",
        "\n",
        "      if index / 10 == 0:\n",
        "        print(f\"Index : {index} >>>=============================>>> Loss : {loss}\")\n",
        "\n",
        "\n",
        "\n",
        "def Eval_Func(dataLoader,model,device):\n",
        "    model.eval()\n",
        "    final_targets = []\n",
        "    final_outputs = []\n",
        "    \n",
        "    for index,batch in enumerate(dataLoader):\n",
        "      ids = batch[\"input_ids\"]\n",
        "      masks = batch[\"attention_masks\"]\n",
        "      token = batch[\"token_type_ids\"]\n",
        "      target = batch[\"targets\"]\n",
        "\n",
        "\n",
        "      ids = ids.to(device,dtype = torch.long)\n",
        "      masks = masks.to(device,dtype = torch.long)\n",
        "      token = token.to(device,dtype = torch.long)\n",
        "      target = target.to(device,dtype = torch.float)\n",
        "\n",
        "      output = model(input_ids = ids,\n",
        "                    attention_mask = masks,\n",
        "                    token_type_ids = token)    \n",
        "      loss = Loss_Func(output,target)\n",
        "\n",
        "      final_targets.extend(target.cpu().detach().numpy().tolist())\n",
        "      final_outputs.extend(torch.sigmoid(output).cpu().detach().numpy().tolist())\n",
        "\n",
        "      return loss, np.vstack(final_outputs),np.vstack(final_targets)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmFLUwan-sOA"
      },
      "source": [
        "\n",
        "def train():\n",
        "\n",
        "  df = pd.read_csv(\"/content/drive/MyDrive/Neuron/toxic/Train_Final.csv\")\n",
        "  \n",
        "  target_cols = df.columns[2:]\n",
        "\n",
        "\n",
        "  Train_Data,Valid_Data,Train_Target,Valid_Target = model_selection.train_test_split(df.comment_text.values,\n",
        "                                                                               df[target_cols].values,\n",
        "                                                                               test_size = .2,\n",
        "                                                                               random_state = 2021,\n",
        "                                                                               shuffle = True)\n",
        "  \n",
        "  Train_dataset = TOXICDATASET(comment_text = Train_Data,target = Train_Target,tokenizer = TOKENIZER,max_len = 128)\n",
        "    \n",
        "  \n",
        "\n",
        "  Train_DataLoader = torch.utils.data.DataLoader(Train_dataset,\n",
        "                                                 batch_size = 16,\n",
        "                                                 sampler = torch.utils.data.RandomSampler(Train_dataset)\n",
        "                                                 )\n",
        "  \n",
        "  Valid_dataset = TOXICDATASET(comment_text = Valid_Data,target = Valid_Target,tokenizer = TOKENIZER,max_len = 128)\n",
        "\n",
        "  Valid_DataLoader = torch.utils.data.DataLoader(Valid_dataset,\n",
        "                                                 batch_size = 8,\n",
        "                                                 sampler = torch.utils.data.SequentialSampler(Valid_dataset)\n",
        "                                              )\n",
        "  \n",
        "  config = \"bert-base-multilingual-uncased\"\n",
        "  model = TOXICMODEL(conf = config)\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "  optimizer_grouped_parameters = [\n",
        "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay_rate': 0.01},\n",
        "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "      'weight_decay_rate': 0.0}\n",
        "  ]\n",
        "  \n",
        "\n",
        "  optimizer = transformers.AdamW(optimizer_grouped_parameters,lr=2e-5,correct_bias=True)\n",
        "\n",
        "  total_steps = int(len(df) / 16 * 3)\n",
        "  scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "  best_loss = np.inf\n",
        "  for epoch in trange(3, desc = \"EPOCHS\"):\n",
        "\n",
        "    Train_Func(dataLoader = Train_DataLoader,optimizer = optimizer, device = device , model = model,scheduler = scheduler)\n",
        "    Valid_loss, output, target = Eval_Func(dataLoader = Valid_DataLoader, model = model,device = device)\n",
        "\n",
        "    if Valid_loss < best_loss:\n",
        "      torch.save(model.state_dict(),\"model.bin\")\n",
        "      Valid_loss = best_loss\n",
        "\n",
        "   \n",
        "\n",
        "  \n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAkbxrcY-sQh",
        "outputId": "a718b963-964d-4727-abfd-693fe5926177"
      },
      "source": [
        "train()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "EPOCHS:   0%|          | 0/3 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Index : 0 >>>=============================>>> Loss : 0.7195372581481934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEPOCHS:  33%|███▎      | 1/3 [00:32<01:05, 32.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Index : 0 >>>=============================>>> Loss : 0.18973109126091003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEPOCHS:  67%|██████▋   | 2/3 [01:06<00:33, 33.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Index : 0 >>>=============================>>> Loss : 0.06324392557144165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCHS: 100%|██████████| 3/3 [01:40<00:00, 33.45s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b15CdwU7iuB4"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}